{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 구현\n",
    "\n",
    "- 먼저, 코드에 사용되는 각종 모듈을 import 해옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gym CartPole 환경 탐색\n",
    "\n",
    "#### env.observation_space\n",
    "\n",
    "- 관측가능한 state 정보를 담고 있는 객체입니다. Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "\n",
    "#### env.action_space\n",
    "\n",
    "- 환경에 적용 가능한 action 정보를 담고 있는 객체 입니다. Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Observation space Max: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Observation space Min: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Action space: Discrete(2)\n",
      "Action space num:  2\n"
     ]
    }
   ],
   "source": [
    "# CartPole-v0 라는 gym 환경을 만들어 env에 저장합니다.\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Box(4,) 는 4개의 요소로 구성된 벡터를 뜻합니다.\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Observation space Max:\", env.observation_space.high)\n",
    "print(\"Observation space Min:\", env.observation_space.low)\n",
    "# Discrete(2) 는 개별적인 두 개의 action이 있음을 뜻합니다.\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Action space num: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. env 와 상호작용하기 : env.reset(), env.step(action)\n",
    "\n",
    "\n",
    "#### env.reset()\n",
    "- env를 original setting으로 초기화 합니다.\n",
    "- 초기화된 observation을 반환합니다.\n",
    "\n",
    "\n",
    "#### obs, reward, done, info =  env.step(action)\n",
    "- action을 env에 적용합니다.\n",
    "- action을 적용한 후의 새로운 state인 obs (env.observation_space), reward (float), done (bool), meta data (dict)를 반환합니다.\n",
    "- 만약 done==True일 경우, episode가 끝난 것이며 env를 다시 초기화해 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 1\n",
      "\n",
      "obs : [ 0.03955248  0.23475609  0.03535985 -0.2719393 ]\n",
      "reward : 1.0\n",
      "done : False\n",
      "info : {}\n",
      "\n",
      "episode reward :  40.0\n",
      "(4,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# reset은 매 episode가 시작할 때마다 호출해야 합니다.\n",
    "obs = env.reset()\n",
    "\n",
    "# random한 action을 뽑아 환경에 적용합니다..\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action: {}\\n\".format(action))\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# info는 현재의 경우 비어있는 dict지만 debugging과 관련된 정보를 포함할 수 있습니다.\n",
    "# reward는 scalar 값 입니다.\n",
    "print(\"obs : {}\\nreward : {}\\ndone : {}\\ninfo : {}\\n\".format(obs, reward, done, info))\n",
    "\n",
    "# 한 episode 에 대한 testing\n",
    "obs, done, ep_reward = env.reset(), False, 0\n",
    "\n",
    "# 대부분의 gym 환경은 다음과 같은 흐름으로 진행됩니다.\n",
    "while True: \n",
    "    action = np.random.randint(2) # action 선택\n",
    "    obs, reward, done, info = env.step(action)  # 환경에 action 적용\n",
    "    ep_reward += reward\n",
    "    if done:  # episode 종료 여부 체크\n",
    "        break\n",
    "        \n",
    "env.close()  \n",
    "# Cartpole에서 reward = episode 동안 지속된 step 을 뜻합니다.\n",
    "print(\"episode reward : \", ep_reward) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DQN 구현 \n",
    "\n",
    "## Network model\n",
    "\n",
    "먼저, 학습에 사용될 neural network model을 구현해 보겠습니다. \n",
    "\n",
    "- Network는 Q value를 approximation하는데 사용됩니다.\n",
    "    - input : state\n",
    "    - output : 입력된 state에서 각 action에 대한 action-value (Q value)\n",
    "\n",
    "**Model(num_of_actions)** \n",
    "- output 출력시 action과 같은 형식이 되도록 설정해줍니다.\n",
    "- Cartpole의 경우 2개의 discrete한 action으로 이루어져 있으므로 Model(2)로 초기화 해줍니다.\n",
    "\n",
    "**action_value(state)**\n",
    "- state를 network에 입력 후 출력된 Q value를 기반으로 Q value가 가장 큰 action(best action)을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, units=[32, 32]):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(units[0], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(units[1], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter 설정\n",
    "\n",
    "학습에 필요한 Hyperparameter를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "units=[32, 32]         # network의 구조. [32, 32]로 설정시 두개의 hidden layer에 32개의 node로 구성된 network가 생성\n",
    "epsilon=1.0            # epsilon의 초기 값\n",
    "min_epsilon=.01        # epsilon의 최솟값\n",
    "epsilon_decay=0.995    # 매 step마다 epsilon이 줄어드는 비율 \n",
    "train_nums=5000        # train이 진행되는 총 step\n",
    "gamma=0.95             # discount factor\n",
    "start_learning = 20\n",
    "\n",
    "buffer_size=5000        # Replay buffer의 size\n",
    "batch_size=8           # Repaly buffer로 부터 가져오는 transition minbatch의 크기\n",
    "\n",
    "target_update_iter=400 # Target network가 update 되는 주기 (step 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network id  140009768936056\n"
     ]
    }
   ],
   "source": [
    "network = Model(2)\n",
    "print(\"network id \", id(network))\n",
    "# network의 optimizer 와 loss 함수를 정의해 줍니다.\n",
    "opt = ko.Adam(learning_rate=.0015)\n",
    "network.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network\n",
    "\n",
    "학습 전, 초기화된 network를 이용해 cartpole 환경을 진행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 48.0\n",
      "1 episode reward : 42.0\n",
      "2 episode reward : 67.0\n",
      "3 episode reward : 30.0\n",
      "4 episode reward : 82.0\n",
      "5 episode reward : 45.0\n",
      "6 episode reward : 38.0\n",
      "7 episode reward : 35.0\n",
      "8 episode reward : 88.0\n",
      "9 episode reward : 42.0\n",
      "mean_reward : 51.70 +/- 19.12\n"
     ]
    }
   ],
   "source": [
    "# test before train\n",
    "epi_rewards = []\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = network.action_value(np.atleast_2d(obs)) \n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 본격적으로 DQN을 구현해 보겠습니다.\n",
    "\n",
    "## Train network\n",
    "\n",
    "state(obs)와 **env.step(action)** 을 통해 얻어지는  next_state, reward, done 값을 이용하여 target value를 계산합니다.\n",
    "\n",
    "**np.amax()** = array에서 가장 큰 값을 반환합니다.  \n",
    "**network.train_on_batch(input, target)** =  input 입력 시 나오는 출력값이 target과 가까워지도록 loss 값을 기반으로 network를 업데이트합니다.\n",
    "\n",
    "\n",
    "Train은 다음과 같은 순서로 진행됩니다.\n",
    "\n",
    "step 1. Select action using epsilon-greedy  \n",
    "step 2. Take step and store transition to replay buffer  \n",
    "step 3. Train Network  \n",
    "step 4. Target network update  \n",
    "\n",
    "- network는 다음과 같은 target value를 기준으로 학습 됩니다.\n",
    "\n",
    " \\begin{aligned}\n",
    "& Y(s, a, r, s') = r + \\gamma \\max_{a'} Q_{\\theta^{-}}(s', a') \\\\\n",
    "& \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Big[ \\big( Y(s, a, r, s') - Q_\\theta(s, a) \\big)^2 \\Big]\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network id  140009768936056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11872a50926546cc8319b485849e762f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    20] avg reward:  14.00  --eps : 0.22 --steps :   305\n",
      "[Episode    40] avg reward:   9.00  --eps : 0.08 --steps :   510\n",
      "[Episode    60] avg reward:  11.00  --eps : 0.03 --steps :   702\n",
      "[Episode    80] avg reward:   8.00  --eps : 0.01 --steps :   889\n",
      "[Episode   100] avg reward:   9.00  --eps : 0.01 --steps :  1118\n",
      "[Episode   120] avg reward:  12.00  --eps : 0.01 --steps :  1317\n",
      "[Episode   140] avg reward:   9.00  --eps : 0.01 --steps :  1506\n",
      "[Episode   160] avg reward:  10.00  --eps : 0.01 --steps :  1694\n",
      "[Episode   180] avg reward:  11.00  --eps : 0.01 --steps :  1880\n",
      "[Episode   200] avg reward:  21.00  --eps : 0.01 --steps :  2140\n",
      "[Episode   220] avg reward:  13.00  --eps : 0.01 --steps :  2356\n",
      "[Episode   240] avg reward:  55.00  --eps : 0.01 --steps :  2955\n",
      "[Episode   260] avg reward:  39.00  --eps : 0.01 --steps :  3514\n",
      "[Episode   280] avg reward:  52.00  --eps : 0.01 --steps :  4425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the initial observation of the agent\n",
    "print(\"network id \", id(network))\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=20)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################  \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    best_action = network.action_value(np.atleast_2d(obs)) \n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    \n",
    "    # target values r + gamma * maxQ(s', a') 계산\n",
    "    # np.amax -> list 에서 가장 큰 값 반환\n",
    "    target_q = reward + gamma * np.amax(network.predict(np.atleast_2d(next_obs))) * (1- done)  # target_q.shape = (1,)\n",
    "    \n",
    "    # get action values from Q network\n",
    "    q_values = network.predict(np.atleast_2d(obs))  # q_values.shape = (1,2)\n",
    " \n",
    "    # update q_value\n",
    "    q_values[0][action] = target_q\n",
    "    \n",
    "    # perform a gradient descent on Q network\n",
    "    # Ths loss measures the mean squared error between prediction and target    \n",
    "    network.train_on_batch(np.atleast_2d(obs), q_values)\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    \n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode +\n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 20 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(epi_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 92.0\n",
      "1 episode reward : 37.0\n",
      "2 episode reward : 34.0\n",
      "3 episode reward : 41.0\n",
      "4 episode reward : 132.0\n",
      "5 episode reward : 129.0\n",
      "6 episode reward : 36.0\n",
      "7 episode reward : 33.0\n",
      "8 episode reward : 36.0\n",
      "9 episode reward : 42.0\n",
      "mean_reward : 61.20 +/- 38.37\n"
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "# After training    \n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0\n",
    "    while not done :\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Experience Replay\n",
    "\n",
    "- 지금까지는 Experience Replay 없이 매 step 마다 얻어지는 transition (s, a, r, s', done) 을 사용해 network를 학습하였습니다. \n",
    "- 이제 Replay Buffer를 적용해 보겠습니다.\n",
    "\n",
    "먼저 Replay Buffer을 다음과 같이 정의해 줍니다.\n",
    "\n",
    "**store(s, a, r, s', done)** = 각 step의 transition 정보를 buffer에 저장합니다.  \n",
    "**sample(batch_size)** = buffer에서 batch_size 크기 만큼의 mini_batch를 sampling 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Buffer를 학습에 포함시켜보겠습니다.\n",
    "\n",
    "step 2 에서 step 실행 후 바로 학습을 진행하지 않고, transtion을 replay buffer에 저장합니다.  \n",
    "step 3 에서 buffer로 부터 minibatch를 sampling후 minibatch를 기반으로 똑같이 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458bcbe45dc948be898d48389bcebd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    20] avg reward:  13.00  --eps : 0.21 --steps :   313\n",
      "[Episode    40] avg reward:  11.00  --eps : 0.07 --steps :   543\n",
      "[Episode    60] avg reward: 162.00  --eps : 0.01 --steps :  1019\n",
      "[Episode    80] avg reward:  44.00  --eps : 0.01 --steps :  1657\n",
      "[Episode   100] avg reward: 108.00  --eps : 0.01 --steps :  2671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(2)\n",
    "opt = ko.Adam(learning_rate=.0015) \n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=20)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################\n",
    "    # select action that maximize Q value f\n",
    "    best_action = network.action_value(np.atleast_2d(obs))  # input the obs to the network model \n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    #TODO : store transition\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    if t > start_learning:\n",
    "        #TODO : get sample from batch\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size) \n",
    "        # calculate target values r + gamma * maxQ(s', a') using Q network\n",
    "        target_q = r_batch + gamma * np.amax(network.predict_on_batch(ns_batch), axis=1) * (1- done_batch)  \n",
    "        # get action values from Q network\n",
    "        q_values = network.predict_on_batch(s_batch)\n",
    "\n",
    "        # update q_value\n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    "\n",
    "         # perform a gradient descent on Q network\n",
    "        # Ths loss measures the mean squared error between prediction and target   \n",
    "        network.train_on_batch(s_batch, q_values)\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 20 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(epi_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 200.0\n",
      "1 episode reward : 200.0\n",
      "2 episode reward : 200.0\n",
      "3 episode reward : 200.0\n",
      "4 episode reward : 200.0\n",
      "5 episode reward : 200.0\n",
      "6 episode reward : 200.0\n",
      "7 episode reward : 200.0\n",
      "8 episode reward : 200.0\n",
      "9 episode reward : 200.0\n",
      "mean_reward : 200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "# After training    \n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done :\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target network\n",
    "\n",
    "- 이제 target network를 추가해 보겠습니다.  \n",
    "\n",
    "stpe 3 에서 target value 계산 시 다음 state의 Q value를 target network로부터 가져옵니다.  \n",
    "step 4 에서 일정 주기마다 target network가 network와 같아지도록 weights를 업데이트 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6a6bac7c6d4810b21231f40fc86f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    20] avg reward:  11.00  --eps : 0.24 --steps :   284\n",
      "[Episode    40] avg reward:  10.00  --eps : 0.09 --steps :   490\n",
      "[Episode    60] avg reward:   8.00  --eps : 0.03 --steps :   677\n",
      "[Episode    80] avg reward:  11.00  --eps : 0.01 --steps :   864\n",
      "[Episode   100] avg reward:   9.00  --eps : 0.01 --steps :  1050\n",
      "[Episode   120] avg reward:  10.00  --eps : 0.01 --steps :  1244\n",
      "[Episode   140] avg reward:  10.00  --eps : 0.01 --steps :  1440\n",
      "[Episode   160] avg reward:  10.00  --eps : 0.01 --steps :  1632\n",
      "[Episode   180] avg reward:   9.00  --eps : 0.01 --steps :  1825\n",
      "[Episode   200] avg reward:  27.00  --eps : 0.01 --steps :  2095\n",
      "[Episode   220] avg reward: 198.00  --eps : 0.01 --steps :  3819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(2)\n",
    "target_network = Model(2)\n",
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0015) \n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=20)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    best_action = network.action_value(np.atleast_2d(obs)) \n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    if t > start_learning:\n",
    "        # target value 계산\n",
    "        # np.amax -> list 에서 가장 큰 값 반환\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        target_q = r_batch + gamma * np.amax(target_network.predict(ns_batch), axis=1) * (1- done_batch)  \n",
    "        q_values = network.predict(s_batch) \n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    "\n",
    "        network.train_on_batch(s_batch, q_values)\n",
    "    \n",
    "    #######################  step 4  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "    if t % target_update_iter == 0:\n",
    "        target_network.set_weights(network.get_weights()) # assign the current network parameters to target network\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 20 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(epi_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 200.0\n",
      "1 episode reward : 200.0\n",
      "2 episode reward : 200.0\n",
      "3 episode reward : 200.0\n",
      "4 episode reward : 200.0\n",
      "5 episode reward : 200.0\n",
      "6 episode reward : 200.0\n",
      "7 episode reward : 200.0\n",
      "8 episode reward : 200.0\n",
      "9 episode reward : 200.0\n",
      "mean_reward : 200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "# After training    \n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0\n",
    "    while not done :\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
