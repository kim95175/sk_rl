{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "Action space: Discrete(3)\n",
      "Sampled action: 0\n",
      "\n",
      "obs : [-0.45583772 -0.00151584]\n",
      "reward : -1.0\n",
      "done : False\n",
      "info : {}\n",
      "\n",
      "episode reward :  -200.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# reset은 매 episode가 시작할 때마다 호출해야 합니다.\n",
    "obs = env.reset()\n",
    "\n",
    "# random한 action을 뽑아 환경에 적용합니다..\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action: {}\\n\".format(action))\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# info는 현재의 경우 비어있는 dict지만 debugging과 관련된 정보를 포함할 수 있습니다.\n",
    "# reward는 scalar 값 입니다.\n",
    "print(\"obs : {}\\nreward : {}\\ndone : {}\\ninfo : {}\\n\".format(obs, reward, done, info))\n",
    "\n",
    "# 한 episode 에 대한 testing\n",
    "obs, done, ep_reward = env.reset(), False, 0\n",
    "\n",
    "# 대부분의 gym 환경은 다음과 같은 흐름으로 진행됩니다.\n",
    "while True: \n",
    "    action = env.action_space.sample() # action 선택\n",
    "    obs, reward, done, info = env.step(action)  # 환경에 action 적용\n",
    "    ep_reward += reward\n",
    "    if done:  # episode 종료 여부 체크\n",
    "        break\n",
    "        \n",
    "env.close()  \n",
    "# Cartpole에서 reward = episode 동안 지속된 step 을 뜻합니다.\n",
    "print(\"episode reward : \", ep_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(64, activation='relu', kernel_initializer='he_uniform')\n",
    "        #self.fc2 = kl.Dense(32, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        #x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action if best_action.shape[0] > 1 else best_action[0] \n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : -200.0\n",
      "1 episode reward : -200.0\n",
      "2 episode reward : -200.0\n",
      "3 episode reward : -200.0\n",
      "4 episode reward : -200.0\n",
      "mean_reward : -200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# test before train\n",
    "epi_rewards = []\n",
    "n_episodes = 5\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        #action = 0\n",
    "        action = np.random.randint(3)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1.0            # epsilon의 초기 값\n",
    "min_epsilon=.02        # epsilon의 최솟값\n",
    "epsilon_decay=0.9997    # 매 step마다 epsilon이 줄어드는 비율 \n",
    "train_nums=50000        # train이 진행되는 총 step\n",
    "gamma=0.95             # discount factor\n",
    "start_learning = 100\n",
    "\n",
    "buffer_size=2000        # Replay buffer의 size\n",
    "batch_size=64        # Repaly buffer로 부터 가져오는 transition minbatch의 크기\n",
    "\n",
    "target_update_iter=200 # Target network가 update 되는 주기 (step 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cd451d951d4532bda697a02438b9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=50000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-305a90f52432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# select action that maximize Q value f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# input the obs to the network model // obs : (4, ) -> np.atleast_2d(obs) : (1, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# e-greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1afc3705e3ee>\u001b[0m in \u001b[0;36maction_value\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# a* = argmax_a' Q(s, a')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbest_action\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \"\"\"\n\u001b[0;32m-> 1652\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4069\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4071\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3219\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \"\"\"\n\u001b[1;32m   2531\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2532\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2497\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfunc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     func_graph = FuncGraph(name, collections=collections,\n\u001b[0;32m--> 868\u001b[0;31m                            capture_by_value=capture_by_value)\n\u001b[0m\u001b[1;32m    869\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFuncGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, collections, capture_by_value)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mouter\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfailing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2726\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names_in_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stack_state_is_thread_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;31m# Functions that will be applied to choose a device if none is specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m     \u001b[0;31m# In TF2.x or after switch_to_thread_local(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(3)\n",
    "target_network = Model(3)\n",
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0015)\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=10)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon and t % 2 ==0:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    \n",
    "    best_action = network.action_value(np.atleast_2d(obs))  # input the obs to the network model // obs : (4, ) -> np.atleast_2d(obs) : (1, 4)\n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon :\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    if next_obs[0] > -0.2:\n",
    "        #print(\"next \", next_obs[0])\n",
    "        if next_obs[0] >= 0.5:\n",
    "            reward += 10\n",
    "        elif next_obs[0] > 0.3:\n",
    "            reward += 3\n",
    "        else:\n",
    "            reward += (next_obs[0] + 0.5)\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    if t > start_learning and t % 5 == 0:\n",
    "        # target value 계산\n",
    "        # np.amax -> list 에서 가장 큰 값 반환\n",
    "        #s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        #best_action_idxes = network.action_value(ns_batch)\n",
    "        #target_q = target_network.predict(ns_batch)\n",
    "        #target_q = r_batch + gamma * target_q[np.arange(target_q.shape[0]), best_action_idxes] *  (1- done_batch)  \n",
    "        #print(r_batch)\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        target_q = r_batch + gamma * np.amax(target_network.predict(ns_batch), axis=1) * (1- done_batch)  \n",
    "        q_values = network.predict(s_batch) \n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    "\n",
    "        network.train_on_batch(s_batch, q_values)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "    if t % target_update_iter == 0:\n",
    "        target_network.set_weights(network.get_weights()) # assign the current network parameters to target network\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 10 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>7.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(avg_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : -153.0\n",
      "1 episode reward : -143.0\n",
      "2 episode reward : -165.0\n",
      "3 episode reward : -171.0\n",
      "4 episode reward : -143.0\n",
      "5 episode reward : -200.0\n",
      "6 episode reward : -200.0\n",
      "7 episode reward : -200.0\n",
      "8 episode reward : -200.0\n",
      "9 episode reward : -142.0\n",
      "mean_reward : -171.70 +/- 24.75\n"
     ]
    }
   ],
   "source": [
    "network.load_weights(\"MountainCar_first_touch\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "# test after train\n",
    "epi_rewards = []\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5938c9e2721c490c9ec7a5a13025cf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=50000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    10] avg reward: -200.00  --eps : 0.05 --steps :  2000\n",
      "[Episode    20] avg reward: -199.50  --eps : 0.03 --steps :  3995\n",
      "[Episode    30] avg reward: -189.20  --eps : 0.02 --steps :  5887\n",
      "[Episode    40] avg reward: -192.50  --eps : 0.02 --steps :  7812\n"
     ]
    }
   ],
   "source": [
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=0.1\n",
    "avg_reward = deque(maxlen=10)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    \n",
    "    best_action = network.action_value(np.atleast_2d(obs))  # input the obs to the network model // obs : (4, ) -> np.atleast_2d(obs) : (1, 4)\n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon :\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    if t > start_learning and t % 5 == 0:\n",
    "        # target value 계산\n",
    "        # np.amax -> list 에서 가장 큰 값 반환\n",
    "        #s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        #best_action_idxes = network.action_value(ns_batch)\n",
    "        #target_q = target_network.predict(ns_batch)\n",
    "        #target_q = r_batch + gamma * target_q[np.arange(target_q.shape[0]), best_action_idxes] *  (1- done_batch)  \n",
    "        #print(r_batch)\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        target_q = r_batch + gamma * np.amax(target_network.predict(ns_batch), axis=1) * (1- done_batch)  \n",
    "        q_values = network.predict(s_batch) \n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    "\n",
    "        network.train_on_batch(s_batch, q_values)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "    if t % target_update_iter == 0:\n",
    "        target_network.set_weights(network.get_weights()) # assign the current network parameters to target network\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 10 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>7.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(avg_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : -159.0\n",
      "1 episode reward : -181.0\n",
      "2 episode reward : -157.0\n",
      "3 episode reward : -181.0\n",
      "4 episode reward : -159.0\n",
      "5 episode reward : -97.0\n",
      "6 episode reward : -92.0\n",
      "7 episode reward : -157.0\n",
      "8 episode reward : -100.0\n",
      "9 episode reward : -92.0\n",
      "mean_reward : -137.50 +/- 35.58\n"
     ]
    }
   ],
   "source": [
    "network = Model(3)\n",
    "opt = ko.Adam(learning_rate=.0015)\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "action = network.action_value(np.atleast_2d(obs))\n",
    "predicted = network.predict(np.atleast_2d(obs))\n",
    "network.train_on_batch(np.atleast_2d(obs), predicted)\n",
    "\n",
    "network.load_weights('MountainCar-trained-dqn.h5')\n",
    "\n",
    "# test after train\n",
    "epi_rewards = []\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = Monitor(env, './video', force=True)\n",
    "epi_reward = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    #action = network.action_value(np.atleast_2d(obs))\n",
    "    action = np.random.randint(2) \n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    epi_reward += reward\n",
    "    if done:\n",
    "        print(\"episode reward : {}\".format(epi_reward))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = io.open('./video/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
