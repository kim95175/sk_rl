{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (8,), float32)\n",
      "Action space: Discrete(4)\n",
      "Sampled action: 1\n",
      "\n",
      "obs : [ 0.01462584  1.4113343   0.73322177 -0.00383807 -0.01468339 -0.1233385\n",
      "  0.          0.        ]\n",
      "reward : 0.5731387702425945\n",
      "done : False\n",
      "info : {}\n",
      "\n",
      "(8,)\n",
      "(1, 8)\n",
      "(1, 8)\n",
      "episode reward :  -91.73367128294416\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# reset은 매 episode가 시작할 때마다 호출해야 합니다.\n",
    "obs = env.reset()\n",
    "\n",
    "# random한 action을 뽑아 환경에 적용합니다..\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action: {}\\n\".format(action))\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# info는 현재의 경우 비어있는 dict지만 debugging과 관련된 정보를 포함할 수 있습니다.\n",
    "# reward는 scalar 값 입니다.\n",
    "print(\"obs : {}\\nreward : {}\\ndone : {}\\ninfo : {}\\n\".format(obs, reward, done, info))\n",
    "\n",
    "# 한 episode 에 대한 testing\n",
    "obs, done, ep_reward = env.reset(), False, 0\n",
    "print(obs.shape)\n",
    "print(obs[None].shape)\n",
    "print(obs.reshape(-1,8).shape)\n",
    "\n",
    "# 대부분의 gym 환경은 다음과 같은 흐름으로 진행됩니다.\n",
    "while True: \n",
    "    action = env.action_space.sample() # action 선택\n",
    "    obs, reward, done, info = env.step(action)  # 환경에 action 적용\n",
    "    ep_reward += reward\n",
    "    if done:  # episode 종료 여부 체크\n",
    "        break\n",
    "        \n",
    "env.close()  \n",
    "# Cartpole에서 reward = episode 동안 지속된 step 을 뜻합니다.\n",
    "print(\"episode reward : \", ep_reward) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "units=[64, 64]         # network의 구조. [32, 32]로 설정시 두개의 hidden layer에 32개의 node로 구성된 network가 생성\n",
    "epsilon=1.0            # epsilon의 초기 값\n",
    "min_epsilon=.01        # epsilon의 최솟값\n",
    "epsilon_decay=0.9995    # 매 step마다 epsilon이 줄어드는 비율 \n",
    "train_nums=100000        # train이 진행되는 총 step\n",
    "gamma=0.99             # discount factor\n",
    "start_learning = 100\n",
    "train_iter = 4\n",
    "\n",
    "buffer_size=10000        # Replay buffer의 size\n",
    "batch_size=65        # Repaly buffer로 부터 가져오는 transition minbatch의 크기\n",
    "\n",
    "target_update_iter=200 # Target network가 update 되는 주기 (step 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, units=[64, 64]):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(units[0], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(units[1], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : -262.51413139563044\n",
      "1 episode reward : -157.8138769723583\n",
      "2 episode reward : -96.32884837926943\n",
      "3 episode reward : -97.30311951324151\n",
      "4 episode reward : -401.6219973851469\n",
      "mean_reward : -203.12 +/- 116.26\n"
     ]
    }
   ],
   "source": [
    "# test before train\n",
    "epi_rewards = []\n",
    "n_episodes = 5\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        #action = 0\n",
    "        action = np.random.randint(4)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e81b110a75b428d98c92f12b4212a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=100000.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode     5] avg reward:  -77.84  --eps : 0.80 --steps :   454\n",
      "[Episode    10] avg reward: -205.03  --eps : 0.61 --steps :   995\n",
      "[Episode    15] avg reward: -258.41  --eps : 0.44 --steps :  1625\n",
      "[Episode    20] avg reward: -235.55  --eps : 0.32 --steps :  2275\n",
      "[Episode    25] avg reward: -234.35  --eps : 0.15 --steps :  3831\n",
      "[Episode    30] avg reward: -214.38  --eps : 0.06 --steps :  5676\n",
      "[Episode    35] avg reward: -207.39  --eps : 0.03 --steps :  6954\n",
      "[Episode    40] avg reward: -162.87  --eps : 0.01 --steps : 11565\n",
      "[Episode    45] avg reward: -127.46  --eps : 0.01 --steps : 16565\n",
      "[Episode    50] avg reward: -101.01  --eps : 0.01 --steps : 21565\n",
      "[Episode    55] avg reward:  -98.50  --eps : 0.01 --steps : 26565\n",
      "[Episode    60] avg reward: -110.21  --eps : 0.01 --steps : 31565\n",
      "[Episode    65] avg reward: -115.45  --eps : 0.01 --steps : 36326\n",
      "[Episode    70] avg reward: -119.19  --eps : 0.01 --steps : 41326\n",
      "[Episode    75] avg reward: -100.28  --eps : 0.01 --steps : 46326\n",
      "[Episode    80] avg reward:  -96.53  --eps : 0.01 --steps : 51326\n",
      "[Episode    85] avg reward: -136.10  --eps : 0.01 --steps : 56326\n",
      "[Episode    90] avg reward: -133.98  --eps : 0.01 --steps : 61326\n",
      "[Episode    95] avg reward: -125.32  --eps : 0.01 --steps : 66326\n",
      "[Episode   100] avg reward: -125.07  --eps : 0.01 --steps : 71326\n",
      "[Episode   105] avg reward: -123.07  --eps : 0.01 --steps : 76326\n",
      "[Episode   110] avg reward: -132.25  --eps : 0.01 --steps : 81326\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(4)\n",
    "target_network = Model(4)\n",
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0005, clipvalue=10.0)  # do gradient clip\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=10)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    \n",
    "    best_action = network.action_value(obs[None])  # input the obs to the network model // obs : (4, ) -> obs[None] : (1, 4)\n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    if t > start_learning and t % train_iter == 0 :\n",
    "        # target value 계산\n",
    "        # np.amax -> list 에서 가장 큰 값 반환\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        best_action_idxes = network.action_value(ns_batch)\n",
    "        target_q = target_network.predict(ns_batch)\n",
    "        target_q = r_batch + gamma * target_q[np.arange(target_q.shape[0]), best_action_idxes] *  (1- done_batch)  \n",
    "        q_values = network.predict(s_batch) \n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    "\n",
    "        network.train_on_batch(s_batch, q_values)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "    if t % target_update_iter == 0:\n",
    "        target_network.set_weights(network.get_weights()) # assign the current network parameters to target network\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 5 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>7.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(avg_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test after train\n",
    "epi_rewards = []\n",
    "n_episodes = 5\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = network.action_value(obs[None]) # Using [None] to extend its dimension (4,) -> (1, 4)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env = Monitor(env, './video', force=True)\n",
    "epi_reward = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = network.action_value(obs[None])\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    epi_reward += reward\n",
    "    if done:\n",
    "        print(\"episode reward : {}\".format(epi_reward))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = io.open('./video/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
