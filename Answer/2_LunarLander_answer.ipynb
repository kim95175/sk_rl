{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (8,), float32)\n",
      "Action space: Discrete(4)\n",
      "Sampled action: 0\n",
      "\n",
      "obs : [ 0.00193148  1.4158367   0.0976758   0.09641916 -0.00220708 -0.02189627\n",
      "  0.          0.        ]\n",
      "reward : 1.5933365319297934\n",
      "done : False\n",
      "info : {}\n",
      "\n",
      "(8,)\n",
      "(1, 8)\n",
      "(1, 8)\n",
      "episode reward :  -353.27305740410327\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# reset은 매 episode가 시작할 때마다 호출해야 합니다.\n",
    "obs = env.reset()\n",
    "\n",
    "# random한 action을 뽑아 환경에 적용합니다..\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action: {}\\n\".format(action))\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# info는 현재의 경우 비어있는 dict지만 debugging과 관련된 정보를 포함할 수 있습니다.\n",
    "# reward는 scalar 값 입니다.\n",
    "print(\"obs : {}\\nreward : {}\\ndone : {}\\ninfo : {}\\n\".format(obs, reward, done, info))\n",
    "\n",
    "# 한 episode 에 대한 testing\n",
    "obs, done, ep_reward = env.reset(), False, 0\n",
    "print(obs.shape)\n",
    "print(obs[None].shape)\n",
    "print(obs.reshape(-1,8).shape)\n",
    "\n",
    "# 대부분의 gym 환경은 다음과 같은 흐름으로 진행됩니다.\n",
    "while True: \n",
    "    action = env.action_space.sample() # action 선택\n",
    "    obs, reward, done, info = env.step(action)  # 환경에 action 적용\n",
    "    ep_reward += reward\n",
    "    if done:  # episode 종료 여부 체크\n",
    "        break\n",
    "        \n",
    "env.close()  \n",
    "# Cartpole에서 reward = episode 동안 지속된 step 을 뜻합니다.\n",
    "print(\"episode reward : \", ep_reward) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "units=[64, 64]         # network의 구조. [32, 32]로 설정시 두개의 hidden layer에 32개의 node로 구성된 network가 생성\n",
    "epsilon=1.0            # epsilon의 초기 값\n",
    "min_epsilon=.01        # epsilon의 최솟값\n",
    "epsilon_decay=0.995    # 매 step마다 epsilon이 줄어드는 비율 \n",
    "train_nums=10000        # train이 진행되는 총 step\n",
    "gamma=0.97             # discount factor\n",
    "start_learning = 100\n",
    "\n",
    "buffer_size=5000        # Replay buffer의 size\n",
    "batch_size=32          # Repaly buffer로 부터 가져오는 transition minbatch의 크기\n",
    "\n",
    "target_update_iter=200 # Target network가 update 되는 주기 (step 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, units=[32, 32]):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(units[0], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(units[1], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : -137.617808977836\n",
      "1 episode reward : -150.30338726822242\n",
      "2 episode reward : -194.83852121823128\n",
      "3 episode reward : -150.4660593637921\n",
      "4 episode reward : -136.98497420417925\n",
      "mean_reward : -154.04 +/- 21.22\n"
     ]
    }
   ],
   "source": [
    "# test before train\n",
    "epi_rewards = []\n",
    "n_episodes = 5\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = 0\n",
    "        #action = np.random.randint(4)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00919f140a4f4c1098c485378b74c8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=10000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode     1] epi reward: -258.21  --eps : 0.58 --steps :   107\n",
      "[Episode     2] epi reward: -239.62  --eps : 0.36 --steps :   202\n",
      "[Episode     3] epi reward: -383.08  --eps : 0.25 --steps :   278\n",
      "[Episode     4] epi reward:   10.82  --eps : 0.15 --steps :   377\n",
      "[Episode     5] epi reward: -161.92  --eps : 0.11 --steps :   442\n",
      "[Episode     6] epi reward: -185.95  --eps : 0.07 --steps :   542\n",
      "[Episode     7] epi reward:  -31.80  --eps : 0.02 --steps :   742\n",
      "[Episode     8] epi reward: -259.78  --eps : 0.01 --steps :   838\n",
      "[Episode     9] epi reward: -265.51  --eps : 0.01 --steps :   971\n",
      "[Episode    10] epi reward: -125.35  --eps : 0.01 --steps :  1058\n",
      "[Episode    11] epi reward: -273.70  --eps : 0.01 --steps :  1194\n",
      "[Episode    12] epi reward: -227.97  --eps : 0.01 --steps :  1449\n",
      "[Episode    13] epi reward:  -90.29  --eps : 0.01 --steps :  1607\n",
      "[Episode    14] epi reward: -114.93  --eps : 0.01 --steps :  1797\n",
      "[Episode    15] epi reward: -122.94  --eps : 0.01 --steps :  2031\n",
      "[Episode    16] epi reward: -103.01  --eps : 0.01 --steps :  2188\n",
      "[Episode    17] epi reward: -165.88  --eps : 0.01 --steps :  2985\n",
      "[Episode    18] epi reward: -105.86  --eps : 0.01 --steps :  3225\n",
      "[Episode    19] epi reward: -105.39  --eps : 0.01 --steps :  3434\n",
      "[Episode    20] epi reward: -133.59  --eps : 0.01 --steps :  3792\n",
      "[Episode    21] epi reward:  -79.98  --eps : 0.01 --steps :  4000\n",
      "[Episode    22] epi reward: -228.26  --eps : 0.01 --steps :  4291\n",
      "[Episode    23] epi reward: -135.49  --eps : 0.01 --steps :  4358\n",
      "[Episode    24] epi reward: -187.93  --eps : 0.01 --steps :  4543\n",
      "[Episode    25] epi reward: -172.60  --eps : 0.01 --steps :  4801\n",
      "[Episode    26] epi reward: -109.09  --eps : 0.01 --steps :  4987\n",
      "[Episode    27] epi reward:  -92.04  --eps : 0.01 --steps :  5173\n",
      "[Episode    28] epi reward: -142.51  --eps : 0.01 --steps :  6173\n",
      "[Episode    29] epi reward: -218.09  --eps : 0.01 --steps :  6839\n",
      "[Episode    30] epi reward: -128.32  --eps : 0.01 --steps :  7839\n",
      "[Episode    31] epi reward:  -93.74  --eps : 0.01 --steps :  8839\n",
      "[Episode    32] epi reward:  -92.59  --eps : 0.01 --steps :  9839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(4)\n",
    "target_network = Model(4)\n",
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0015, clipvalue=10.0)  # do gradient clip\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    \n",
    "    best_action = network.action_value(obs[None])  # input the obs to the network model // obs : (4, ) -> obs[None] : (1, 4)\n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    if t > start_learning:\n",
    "        # target value 계산\n",
    "        # np.amax -> list 에서 가장 큰 값 반환\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        target_q = r_batch + gamma * np.amax(target_network.predict(ns_batch), axis=1) * (1- done_batch)  \n",
    "        q_values = network.predict(s_batch) \n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    "\n",
    "        network.train_on_batch(s_batch, q_values)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "    if t % target_update_iter == 0:\n",
    "        target_network.set_weights(network.get_weights()) # assign the current network parameters to target network\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        if epi % 1 == 0:\n",
    "            print(\"[Episode {:>5}] epi reward: {:>7.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, epi_reward, epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : -290.3500182318668\n",
      "1 episode reward : -151.71938543512994\n",
      "2 episode reward : -73.44159925655296\n",
      "3 episode reward : -72.34356447080194\n",
      "4 episode reward : -45.80558311400055\n",
      "mean_reward : -126.73 +/- 89.15\n"
     ]
    }
   ],
   "source": [
    "# test after train\n",
    "epi_rewards = []\n",
    "n_episodes = 5\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = network.action_value(obs[None]) # Using [None] to extend its dimension (4,) -> (1, 4)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
