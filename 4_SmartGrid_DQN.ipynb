{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmartGrid with DQN\n",
    "\n",
    "\n",
    "### SmartGrid with DQN 학습 목표\n",
    "\n",
    "**주제** : SmartGrid Envrionment 에 DQN 적용해 보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from grid2op.PlotGrid import PlotMatplot \n",
    "from tqdm import tqdm, notebook \n",
    "from grid_agent import GridAgent\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk/.conda/envs/prac2/lib/python3.6/site-packages/grid2op/MakeEnv/Make.py:255: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "env = grid2op.make(\"rte_case5_example\", test=True)\n",
    "plot_helper = PlotMatplot(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SmartGrid 환경은 gym의 구조와 유사하지만, state(observation)와 action에 차이가 있습니다.\n",
    "그 부분을 먼저 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter\n",
    "\n",
    "SmartGrid 환경은 독자적인 state와 action의 형식을 사용합니다. 위와 같이 일반적인 Gym과 같은 형식으로 test하는 것은 가능하지만, 이 전에 구현한 DQN을 통해 학습하는 것은 불가능하다는 사실을 알 수 있습니다. \n",
    "\n",
    "사전에 구현한 DQN을 이용해 Smart Grid 환경에서 학습을 진행하기 위해서는, state와 action을 network model에 맞춰 변환시켜 줘야 하며, Smart Grid 환경은 이를 위한 Converter 함수를 제공합니다.\n",
    "\n",
    "사전에 구현된 Converter 함수를 이용해 observation과 action을 변환해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid 환경에 맞춰 Conveter를 추가한 Agent 사용.\n",
    "agent = GridAgent(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation space\n",
    "\n",
    "**agent.convert_obs(obs)**\n",
    "\n",
    "convert_obs 함수는 grid 환경의 observation을 input으로 받아 학습에 적합한 gym 스타일의 observation (ndarray)로 변환시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<grid2op.Space.GridObjects.CompleteObservation_rte_case5_example object at 0x7efe6ccf95f8>\n",
      "(39,)\n"
     ]
    }
   ],
   "source": [
    "### Observation space\n",
    "grid_obs = env.reset()\n",
    "print(grid_obs)\n",
    "\n",
    "gym_obs = agent.convert_obs(grid_obs)\n",
    "print(gym_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "**agent.id_to_act(act(int))**\n",
    "\n",
    "id_to_act 함수는 int형의 action을 input으로 받아, grid 환경에 사용 가능한 action으로 변환시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of action =  68\n",
      "int-like action =  14\n",
      "grid_ action \n",
      " This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - Change the bus of the following element:\n",
      "\t \t - switch bus of line (origin) 3 [on substation 0]\n",
      "\t \t - switch bus of generator 0 [on substation 0]\n",
      "\t - NOT force any particular bus configuration\n"
     ]
    }
   ],
   "source": [
    "### Action space\n",
    "print(\"number of action = \", agent.num_actions)\n",
    "\n",
    "gym_act = np.random.randint(agent.num_actions)\n",
    "print(\"int-like action = \", gym_act)\n",
    "\n",
    "grid_act = agent.id_to_act(gym_act)\n",
    "print(\"grid_ action \\n\", grid_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size=5000\n",
    "learning_rate=0.01\n",
    "epsilon=1.0\n",
    "epsilon_decay=0.99\n",
    "min_epsilon=0.01\n",
    "gamma=0.98\n",
    "batch_size=16\n",
    "target_update_iter=400\n",
    "train_nums=5000\n",
    "start_learning = 40\n",
    "max_iter=200\n",
    "\n",
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, units=[32, 32]):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(units[0], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(units[1], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    \n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0]\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Grid Environment Test\n",
    "\n",
    "3가지 action type을 test해보겠습니다.\n",
    "\n",
    "1. **network** : network에 state를 입력하여 출력된 output을 action으로 취합니다. 현재는 network가 학습되기 전이므로, 랜덤한 action을 수행하게 됩니다.\n",
    "\n",
    "2. **random** : random한 action을 수행합니다.\n",
    "\n",
    "3. **do_nothing** : action 0 번, 전력망 환경에 어떤 영향도 끼치지 않는 action을 수행합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f94b128240d4a219bf1b36b65d595a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Smart grid [ network ] agent', max=2016.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[24, 24, 24, 24, 24, 24, 24, 24, 24, 24]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e47e292ab54c468aca1c85fbf74967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Smart grid [ random ] agent', max=2016.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[49, 47, 61, 6, 59, 15, 33, 63, 12, 4]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9434220632764e92afdbbead28a37ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Smart grid [ do_nothing ] agent', max=2016.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "network = Model(agent.num_actions)\n",
    "action_types = [\"network\", \"random\", \"do_nothing\"]\n",
    "max_steps = env.chronics_handler.max_timestep()\n",
    "\n",
    "# test before train\n",
    "for action_type in action_types:\n",
    "    done = False\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    actions = []\n",
    "    msg = \"Smart grid [ {} ] agent\".format(action_type)\n",
    "    for t in notebook.tqdm(range(max_steps), desc=msg):\n",
    "        converted_obs = agent.convert_obs(obs)\n",
    "        if action_type == \"network\":   # select action using inital network \n",
    "            action = network.action_value(converted_obs[None])\n",
    "        elif action_type == \"random\":  # select random action\n",
    "            action = np.random.randint(agent.num_actions) # select do-nothing action ( action number 0)\n",
    "        elif action_type == \"do_nothing\":\n",
    "            action = 0\n",
    "        actions.append(action)\n",
    "        converted_act = agent.id_to_act(action)\n",
    "        obs, reward, done, _ = env.step(converted_act)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    print(actions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb009e9181b43bab5d08a924ac0cffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode     1] epi duration:   6.00  --eps : 0.94 --steps :     6\n",
      "[Episode     2] epi duration: 200.00  --eps : 0.13 --steps :   206\n",
      "[Episode     3] epi duration: 200.00  --eps : 0.02 --steps :   406\n",
      "[Episode     4] epi duration:  18.00  --eps : 0.01 --steps :   424\n",
      "[Episode     5] epi duration:  85.00  --eps : 0.01 --steps :   509\n",
      "[Episode     6] epi duration:  13.00  --eps : 0.01 --steps :   522\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b165f5eca334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgrid_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_act\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Excute action in the env to return s'(next state), r, done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mgym_next_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym_next_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grid/lib/python3.6/site-packages/grid2op/Environment/BaseEnv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mexcept_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0mdetailed_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0minit_disp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_redispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m         \u001b[0mattack_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mlines_attacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubs_attacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "network = Model(agent.num_actions)\n",
    "target_network = Model(agent.num_actions)\n",
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0015, clipvalue=10.0)  # do gradient clip\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_duration = 0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    \n",
    "    \n",
    "\n",
    "    #######################  step 3  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "\n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if (epi_duration >= max_iter) or done:\n",
    "        epi += 1 # num of episode \n",
    "        print(\"[Episode {:>5}] epi duration: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, epi_duration, epsilon, t))\n",
    "        obs, done, epi_duration = env.reset(), False, 0  # Environmnet reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f65b00fa97945989fe8de768a4ff9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665c467348eb4491ad3eee4a06f78b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b12b69d74b4916b864e13cfc80d260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c67c72c1844718854d4b1fac7195be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3355ab47a04a34b8c62980c98eda55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_steps = env.chronics_handler.max_timestep()\n",
    "\n",
    "# test before train\n",
    "for i in range(5):\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    actions = []\n",
    "    for t in notebook.tqdm(range(max_steps), desc=\"\"):\n",
    "        converted_obs = agent.convert_obs(obs)\n",
    "        action = network.action_value(converted_obs[None])\n",
    "        actions.append(action)\n",
    "        converted_act = agent.id_to_act(action)\n",
    "        obs, reward, done, _ = env.step(converted_act)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
