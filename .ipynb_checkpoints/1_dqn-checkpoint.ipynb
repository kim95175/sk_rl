{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 구현\n",
    "\n",
    "- 먼저, 코드에 사용되는 각종 모듈을 import 해옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gym CartPole 환경 탐색\n",
    "\n",
    "#### env.observation_space\n",
    "\n",
    "- 관측가능한 state 정보를 담고 있는 객체입니다. Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "\n",
    "#### env.action_space\n",
    "\n",
    "- 환경에 적용 가능한 action 정보를 담고 있는 객체 입니다. Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Observation space Max: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Observation space Min: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "\n",
      "Action space: Discrete(2)\n",
      "Action space num:  2\n"
     ]
    }
   ],
   "source": [
    "# CartPole-v0 라는 gym 환경을 만들어 env에 저장합니다.\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Box(4,) 는 4개의 요소로 구성된 벡터를 뜻합니다.\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Observation space Max:\", env.observation_space.high)\n",
    "print(\"Observation space Min:\", env.observation_space.low)\n",
    "\n",
    "# Discrete(2) 는 개별적인 두 개의 action이 있음을 뜻합니다.\n",
    "print(\"\\nAction space:\", env.action_space)\n",
    "print(\"Action space num: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render\n",
    "\n",
    "실제 환경이 동작하는 모습을 보겠습니다.\n",
    "\n",
    "먼저 학습이 되지 않았을 경우 입니다. ( reward = 19 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./video/cartpole_before.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "향후 작성할 코드를 통해 학습한 결과입니다. (reward = 199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = io.open('./video/cartpole_after.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. env 와 상호작용하기 : env.reset(), env.step(action)\n",
    "\n",
    "\n",
    "#### env.reset()\n",
    "- env를 original setting으로 초기화 합니다.\n",
    "- 초기화된 observation을 반환합니다.\n",
    "\n",
    "\n",
    "#### obs, reward, done, info =  env.step(action)\n",
    "- action을 env에 적용합니다.\n",
    "- action을 적용한 후의 새로운 state인 obs (env.observation_space), reward (float), done (bool), meta data (dict)를 반환합니다.\n",
    "- 만약 done==True일 경우, episode가 끝난 것이며 env를 다시 초기화해 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0\n",
      "\n",
      "obs : [ 0.03037678 -0.18780104 -0.00950443  0.2941801 ]\n",
      "reward : 1.0\n",
      "done : False\n",
      "info : {}\n",
      "\n",
      "episode reward :  52.0\n"
     ]
    }
   ],
   "source": [
    "# reset은 매 episode가 시작할 때마다 호출해야 합니다.\n",
    "obs = env.reset()\n",
    "\n",
    "# random한 action을 뽑아 환경에 적용합니다..\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action: {}\\n\".format(action))\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# info는 현재의 경우 비어있는 dict지만 debugging과 관련된 정보를 포함할 수 있습니다.\n",
    "# reward는 scalar 값 입니다.\n",
    "print(\"obs : {}\\nreward : {}\\ndone : {}\\ninfo : {}\\n\".format(obs, reward, done, info))\n",
    "\n",
    "# 한 episode 에 대한 testing\n",
    "obs, done, ep_reward = env.reset(), False, 0\n",
    "\n",
    "# 대부분의 gym 환경은 다음과 같은 흐름으로 진행됩니다.\n",
    "while True: \n",
    "    #action = np.random.randint(2) # action 선택\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    obs, reward, done, info = env.step(action)  # 환경에 action 적용\n",
    "    \n",
    "    ep_reward += reward\n",
    "    \n",
    "    if done:  # episode 종료 여부 체크\n",
    "        break\n",
    "        \n",
    "env.close()  \n",
    "# Cartpole에서 reward = episode 동안 지속된 step 을 뜻합니다.\n",
    "print(\"episode reward : \", ep_reward) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DQN 구현 \n",
    "\n",
    "## Network model\n",
    "\n",
    "먼저, 학습에 사용될 neural network model을 구현해 보겠습니다. \n",
    "\n",
    "- Network는 Q value를 approximation하는데 사용됩니다.\n",
    "    - input : state\n",
    "    - output : 입력된 state에서 각 action에 대한 action-value (Q value)\n",
    "\n",
    "**Model(num_of_actions)** \n",
    "- output 출력시 action과 같은 형식이 되도록 설정해줍니다.\n",
    "- Cartpole의 경우 2개의 discrete한 action으로 이루어져 있으므로 Model(2)로 초기화 해줍니다.\n",
    "\n",
    "**action_value(state)**\n",
    "- state를 network에 입력 후 출력된 Q value를 기반으로 Q value가 가장 큰 action(best action)을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, units=[32, 32]):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(units[0], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(units[1], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter 설정\n",
    "\n",
    "학습에 필요한 Hyperparameter를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "units=[32, 32]         # network의 구조. [32, 32]로 설정시 두개의 hidden layer에 32개의 node로 구성된 network가 생성\n",
    "epsilon=1.0            # epsilon의 초기 값\n",
    "min_epsilon=.01        # epsilon의 최솟값\n",
    "epsilon_decay=0.995    # 매 step마다 epsilon이 줄어드는 비율 \n",
    "train_nums=5000        # train이 진행되는 총 step\n",
    "gamma=0.95             # discount factor\n",
    "start_learning = 20\n",
    "\n",
    "buffer_size=5000        # Replay buffer의 size\n",
    "batch_size=8           # Repaly buffer로 부터 가져오는 transition minbatch의 크기\n",
    "\n",
    "target_update_iter=400 # Target network가 update 되는 주기 (step 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network id  140143379970200\n"
     ]
    }
   ],
   "source": [
    "network = # TODO\n",
    "\n",
    "print(\"network id \", id(network))\n",
    "# network의 optimizer 와 loss 함수를 정의해 줍니다.\n",
    "opt = ko.Adam(learning_rate=.0015)\n",
    "network.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network\n",
    "\n",
    "학습 전, 초기화된 network를 이용해 cartpole 환경을 진행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-52d12b15d7a2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-52d12b15d7a2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    action = # TODO : get action from network\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# test before train\n",
    "epi_rewards = []\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done:\n",
    "        action = # TODO : get action from network\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        epi_reward += reward\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 본격적으로 DQN을 구현해 보겠습니다.\n",
    "\n",
    "## Train network\n",
    "\n",
    "state(obs)와 **env.step(action)** 을 통해 얻어지는  next_state, reward, done 값을 이용하여 target value를 계산합니다.\n",
    "\n",
    "**np.amax()** = array에서 가장 큰 값을 반환합니다.  \n",
    "**network.train_on_batch(input, target)** =  input 입력 시 나오는 출력값이 target과 가까워지도록 loss 값을 기반으로 network를 업데이트합니다.\n",
    "\n",
    "\n",
    "Train은 다음과 같은 순서로 진행됩니다.\n",
    "\n",
    "step 1. Select action using epsilon-greedy  \n",
    "step 2. Take step and store transition to replay buffer  \n",
    "step 3. Train Network  \n",
    "step 4. Target network update  \n",
    "\n",
    "- network는 다음과 같은 target value를 기준으로 학습 됩니다.\n",
    "\n",
    " \\begin{aligned}\n",
    "& Y(s, a, r, s') = r + \\gamma \\max_{a'} Q_{\\theta^{-}}(s', a') \\\\\n",
    "& \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Big[ \\big( Y(s, a, r, s') - Q_\\theta(s, a) \\big)^2 \\Big]\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network id  140503010370840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415cd285ff7a491abbc576c7443be628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    20] epi reward:  11.00  --eps : 0.22 --steps :   300\n",
      "[Episode    40] epi reward:   9.00  --eps : 0.08 --steps :   504\n",
      "[Episode    60] epi reward:   9.00  --eps : 0.03 --steps :   689\n",
      "[Episode    80] epi reward:  10.00  --eps : 0.01 --steps :   878\n",
      "[Episode   100] epi reward:   9.00  --eps : 0.01 --steps :  1060\n",
      "[Episode   120] epi reward:   9.00  --eps : 0.01 --steps :  1246\n",
      "[Episode   140] epi reward:  10.00  --eps : 0.01 --steps :  1436\n",
      "[Episode   160] epi reward:   9.00  --eps : 0.01 --steps :  1622\n",
      "[Episode   180] epi reward:  10.00  --eps : 0.01 --steps :  1808\n",
      "[Episode   200] epi reward:   9.00  --eps : 0.01 --steps :  1989\n",
      "[Episode   220] epi reward:   8.00  --eps : 0.01 --steps :  2178\n",
      "[Episode   240] epi reward:   9.00  --eps : 0.01 --steps :  2370\n",
      "[Episode   260] epi reward:  10.00  --eps : 0.01 --steps :  2555\n",
      "[Episode   280] epi reward:   9.00  --eps : 0.01 --steps :  2744\n",
      "[Episode   300] epi reward:  11.00  --eps : 0.01 --steps :  2929\n",
      "[Episode   320] epi reward:   8.00  --eps : 0.01 --steps :  3116\n",
      "[Episode   340] epi reward:  10.00  --eps : 0.01 --steps :  3307\n",
      "[Episode   360] epi reward:  10.00  --eps : 0.01 --steps :  3493\n",
      "[Episode   380] epi reward:  10.00  --eps : 0.01 --steps :  3682\n",
      "[Episode   400] epi reward:  10.00  --eps : 0.01 --steps :  3875\n",
      "[Episode   420] epi reward:  11.00  --eps : 0.01 --steps :  4065\n",
      "[Episode   440] epi reward:   9.00  --eps : 0.01 --steps :  4253\n",
      "[Episode   460] epi reward:   8.00  --eps : 0.01 --steps :  4431\n",
      "[Episode   480] epi reward:   9.00  --eps : 0.01 --steps :  4618\n",
      "[Episode   500] epi reward:  10.00  --eps : 0.01 --steps :  4809\n",
      "[Episode   520] epi reward:   9.00  --eps : 0.01 --steps :  4997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the initial observation of the agent\n",
    "print(\"network id \", id(network))\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=20)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################  \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    # np.atleast_2d(obs) : obs : (4,)  -> (1, 4)   - 1개 짜리 배치 형태로 바꿔줌.\n",
    "    best_action = network.action_value(np.atleast_2d(obs))  # input the obs to the network model\n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        action = best_action\n",
    "    \n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    \n",
    "    # target values r + gamma * maxQ(s', a') 계산\n",
    "    # np.amax -> list 에서 가장 큰 값 반환\n",
    "    target_q = # TODO\n",
    "    \n",
    "    # get action values from Q network\n",
    "    q_values = # TODO\n",
    "    \n",
    "    # update q_value\n",
    "    q_values[0][action] = target_q\n",
    "    \n",
    "    # perform a gradient descent on Q network\n",
    "    # Ths loss measures the mean squared error between prediction and target    \n",
    "    network.train_on_batch(np.atleast_2d(obs), q_values)\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    \n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode +\n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 20 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(epi_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 11.0\n",
      "1 episode reward : 10.0\n",
      "2 episode reward : 9.0\n",
      "3 episode reward : 9.0\n",
      "4 episode reward : 9.0\n",
      "5 episode reward : 10.0\n",
      "6 episode reward : 8.0\n",
      "7 episode reward : 10.0\n",
      "8 episode reward : 10.0\n",
      "9 episode reward : 10.0\n",
      "mean_reward : 9.60 +/- 0.80\n"
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "# After training    \n",
    "for i in range(10):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done :\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Experience Replay\n",
    "\n",
    "- 지금까지는 Experience Replay 없이 매 step 마다 얻어지는 transition (s, a, r, s', done) 을 사용해 network를 학습하였습니다. \n",
    "- 이제 Replay Buffer를 적용해 보겠습니다.\n",
    "\n",
    "먼저 Replay Buffer을 다음과 같이 정의해 줍니다.\n",
    "\n",
    "**store(s, a, r, s', done)** = 각 step의 transition 정보를 buffer에 저장합니다.  \n",
    "**sample(batch_size)** = buffer에서 batch_size 크기 만큼의 mini_batch를 sampling 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        assert batch_size < self. count\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        \n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Buffer를 학습에 포함시켜보겠습니다.\n",
    "\n",
    "step 2 에서 step 실행 후 바로 학습을 진행하지 않고, transtion을 replay buffer에 저장합니다.  \n",
    "step 3 에서 buffer로 부터 minibatch를 sampling후 minibatch를 기반으로 똑같이 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44619dac398440e68b41dc7c3b4a41c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    20] epi reward:  12.00  --eps : 0.29 --steps :   244\n",
      "[Episode    40] epi reward:  12.00  --eps : 0.10 --steps :   456\n",
      "[Episode    60] epi reward:  28.00  --eps : 0.03 --steps :   682\n",
      "[Episode    80] epi reward: 172.00  --eps : 0.01 --steps :  2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : initialize replay buffer\n",
    "replay_buffer = #TODO\n",
    "\n",
    "network = Model(2)\n",
    "opt = ko.Adam(learning_rate=.0015) \n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "print(\"network id \", id(network))\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "avg_reward = deque(maxlen=20)\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################  \n",
    "\n",
    "    # select action that maximize Q value f\n",
    "    best_action = network.action_value(np.atleast_2d(obs)) \n",
    "    \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        action = best_action   # with prob. epsilon, select a random action\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    #TODO : store transition (s, a, r, s', done)\n",
    "    \n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    \n",
    "    if t > start_learning:\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = #TODO : get sample from batch\n",
    "        target_q = #TODO : Calculate targe value   \n",
    "        #target_q = reward + gamma * np.amax(network.predict(np.atleast_2d(next_obs))) * (1- done)  \n",
    "        \n",
    "        # get action values from Q network\n",
    "        q_values = # TODO \n",
    "        #q_values = network.predict(np.atleast_2d(obs)) # TODO \n",
    "    \n",
    "    \n",
    "        for i, action in enumerate(a_batch):\n",
    "            # TODO :  Upadet q_values 'in batch'\n",
    "            #q_values[0][action] = target_q\n",
    "      \n",
    "        network.train_on_batch(# TODO, q_values)\n",
    "        #network.train_on_batch(np.atleast_2d(obs), q_values)\n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    \n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 \n",
    "        avg_reward.append(epi_reward)\n",
    "        if epi % 20 == 0:\n",
    "            print(\"[Episode {:>5}] avg reward: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, np.mean(epi_reward), epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 10.0\n",
      "1 episode reward : 10.0\n",
      "2 episode reward : 9.0\n",
      "3 episode reward : 10.0\n",
      "4 episode reward : 8.0\n",
      "5 episode reward : 10.0\n",
      "6 episode reward : 9.0\n",
      "7 episode reward : 9.0\n",
      "8 episode reward : 11.0\n",
      "9 episode reward : 8.0\n",
      "mean_reward : 9.40 +/- 0.92\n"
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "# After training    \n",
    "for i in range(10):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0 \n",
    "    while not done :\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target network\n",
    "\n",
    "- 이제 target network를 추가해 보겠습니다.  \n",
    "\n",
    "stpe 3 에서 target value 계산 시 다음 state의 Q value를 target network로부터 가져옵니다.  \n",
    "step 4 에서 일정 주기마다 target network가 network와 같아지도록 weights를 업데이트 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10fe977b84e4e6da06216c3d8dd7a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train with DQN', max=5000.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    20] epi reward:  15.00  --eps : 0.25 --steps :   278\n",
      "[Episode    40] epi reward:  11.00  --eps : 0.08 --steps :   492\n",
      "[Episode    60] epi reward:   9.00  --eps : 0.03 --steps :   694\n",
      "[Episode    80] epi reward:   9.00  --eps : 0.01 --steps :   897\n",
      "[Episode   100] epi reward:  10.00  --eps : 0.01 --steps :  1092\n",
      "[Episode   120] epi reward:  17.00  --eps : 0.01 --steps :  1314\n",
      "[Episode   140] epi reward:  15.00  --eps : 0.01 --steps :  1547\n",
      "[Episode   160] epi reward:  14.00  --eps : 0.01 --steps :  1791\n",
      "[Episode   180] epi reward:  17.00  --eps : 0.01 --steps :  2071\n",
      "[Episode   200] epi reward:  14.00  --eps : 0.01 --steps :  2402\n",
      "[Episode   220] epi reward:  98.00  --eps : 0.01 --steps :  3758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(2)\n",
    "# TODO : initialize target_network \n",
    "target_network =\n",
    "target_network.set_weights(#TODO) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0015)\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_reward = 0.0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################\n",
    "    # select action that maximize Q value f\n",
    "    best_action = network.action_value(np.atleast_2d(obs))  \n",
    "    # e-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        action = best_action  \n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)    # Excute action in the env to return s'(next state), r, done\n",
    "    epi_reward += reward\n",
    "    replay_buffer.store(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    \n",
    "    if t > start_learning:\n",
    "        \n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = replay_buffer.sample(batch_size) \n",
    "        # TODO :\n",
    "        # calculate target values r + gamma * maxQ(s', a') using 'Target Q network'\n",
    "        # target_q = r_batch + gamma * np.amax(network.predict_on_batch(ns_batch), axis=1) * (1- done_batch)  \n",
    "        \n",
    "        q_values = network.predict(s_batch)\n",
    "\n",
    "        for i, action in enumerate(a_batch):\n",
    "            q_values[i][action] = target_q[i]\n",
    " \n",
    "        network.train_on_batch(s_batch, q_values)\n",
    "    \n",
    "    #######################  step 4  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "    if t % target_update_iter == 0:\n",
    "        # TODO : upadet target network\n",
    " \n",
    " \n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if done:\n",
    "        epi += 1 # num of episode \n",
    "        if epi % 20 == 0:\n",
    "            print(\"[Episode {:>5}] epi reward: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, epi_reward, epsilon, t))\n",
    "        obs, done, epi_reward = env.reset(), False, 0.0  # Environmnet reset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode reward : 200.0\n",
      "1 episode reward : 200.0\n",
      "2 episode reward : 200.0\n",
      "3 episode reward : 200.0\n",
      "4 episode reward : 200.0\n",
      "5 episode reward : 200.0\n",
      "6 episode reward : 200.0\n",
      "7 episode reward : 200.0\n",
      "8 episode reward : 200.0\n",
      "9 episode reward : 200.0\n",
      "mean_reward : 200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "# After training    \n",
    "for i in range(10):\n",
    "    obs, done, epi_reward = env.reset(), False, 0.0\n",
    "    while not done :\n",
    "        action = network.action_value(np.atleast_2d(obs))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "    print(\"{} episode reward : {}\".format(i, epi_reward))\n",
    "    epi_rewards.append(epi_reward)\n",
    "\n",
    "mean_reward = np.mean(epi_rewards)\n",
    "std_reward = np.std(epi_rewards)\n",
    "\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = Monitor(env, './video', force=True)\n",
    "epi_reward = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = network.action_value(np.atleast_2d(obs))\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    epi_reward += reward\n",
    "    if done:\n",
    "        print(\"episode reward : {}\".format(epi_reward))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = io.open('./video/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
