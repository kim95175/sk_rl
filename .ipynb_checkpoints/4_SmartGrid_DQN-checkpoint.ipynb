{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmartGrid with DQN\n",
    "\n",
    "\n",
    "### SmartGrid with DQN 학습 목표\n",
    "\n",
    "**주제** : SmartGrid Envrionment 에 DQN 적용해 보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from grid2op.PlotGrid import PlotMatplot \n",
    "from tqdm import tqdm, notebook \n",
    "from grid_agent import GridAgent\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid2op.make(\"rte_case5_example\", test=True)\n",
    "plot_helper = PlotMatplot(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SmartGrid 환경은 gym의 구조와 유사하지만, state(observation)와 action에 차이가 있습니다.\n",
    "그 부분을 먼저 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter\n",
    "\n",
    "SmartGrid 환경은 독자적인 state와 action의 형식을 사용합니다. 위와 같이 일반적인 Gym과 같은 형식으로 test하는 것은 가능하지만, 이 전에 구현한 DQN을 통해 학습하는 것은 불가능하다는 사실을 알 수 있습니다. \n",
    "\n",
    "사전에 구현한 DQN을 이용해 Smart Grid 환경에서 학습을 진행하기 위해서는, state와 action을 network model에 맞춰 변환시켜 줘야 하며, Smart Grid 환경은 이를 위한 Converter 함수를 제공합니다.\n",
    "\n",
    "사전에 구현된 Converter 함수를 이용해 observation과 action을 변환해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid 환경에 맞춰 Conveter를 추가한 Agent 사용.\n",
    "agent = GridAgent(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation space\n",
    "\n",
    "**agent.convert_obs(obs)**\n",
    "\n",
    "convert_obs 함수는 grid 환경의 observation을 input으로 받아 학습에 적합한 gym 스타일의 observation (ndarray)로 변환시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Observation space\n",
    "grid_obs = env.reset()\n",
    "print(grid_obs)\n",
    "\n",
    "gym_obs = agent.convert_obs(grid_obs)\n",
    "print(gym_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "**agent.id_to_act(act(int))**\n",
    "\n",
    "id_to_act 함수는 int형의 action을 input으로 받아, grid 환경에 사용 가능한 action으로 변환시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Action space\n",
    "print(\"number of action = \", agent.num_actions)\n",
    "\n",
    "gym_act = np.random.randint(agent.num_actions)\n",
    "print(\"int-like action = \", gym_act)\n",
    "\n",
    "grid_act = agent.id_to_act(gym_act)\n",
    "print(\"grid_ action \\n\", grid_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action을 모두 출력해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(agent.num_actions):\n",
    "    print(i, agent.id_to_act(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "from collections import deque\n",
    "from tqdm import tqdm, notebook  # 학습 과정을 더 깔끔하게 보여주는 library 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size=5000\n",
    "learning_rate=0.01\n",
    "epsilon=1.0\n",
    "epsilon_decay=0.99\n",
    "min_epsilon=0.01\n",
    "gamma=0.98\n",
    "batch_size=16\n",
    "target_update_iter=400\n",
    "train_nums=5000\n",
    "start_learning = 40\n",
    "max_iter=200   # 한 에피소드 내에서 최대 step 수를 제한합니다.\n",
    "\n",
    "# Neural Network Model \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, units=[32, 32]):\n",
    "        super().__init__()\n",
    "        self.fc1 = kl.Dense(units[0], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(units[1], activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    \n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # return best action that maximize action-value (Q) from network\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0]\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "    # store transition of each step in replay buffer\n",
    "    def store(self, s, a, r, next_s, d):\n",
    "        experience = (s, a, r, d, next_s)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    # Sample random minibatch of transtion\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = map(np.array, list(zip(*batch)))\n",
    "        return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Grid Environment Test\n",
    "\n",
    "2가지 action type을 test해보겠습니다.\n",
    "\n",
    "1. **random** : random한 action을 수행합니다.\n",
    "\n",
    "2. **do_nothing** : action 0 번, 전력망 환경에 어떤 영향도 끼치지 않는 do-nothing action을 수행합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_types = [\"random\", \"do_nothing\"]\n",
    "max_steps = env.chronics_handler.max_timestep()\n",
    "\n",
    "# test before train\n",
    "for action_type in action_types:\n",
    "    done = False\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    actions = []\n",
    "    msg = \"Smart grid [ {} ] agent\".format(action_type)\n",
    "    for t in notebook.tqdm(range(max_steps), desc=msg):\n",
    "        if action_type == \"random\":  # select random action\n",
    "            action = np.random.randint(agent.num_actions) # select do-nothing action ( action number 0)\n",
    "        elif action_type == \"do_nothing\":\n",
    "            action = 0\n",
    "        actions.append(action)\n",
    "        converted_act = agent.id_to_act(action)\n",
    "        obs, reward, done, _ = env.step(converted_act)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    print(actions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "network = Model(agent.num_actions)\n",
    "target_network = Model(agent.num_actions)\n",
    "target_network.set_weights(network.get_weights()) # initialize target network weight \n",
    "opt = ko.Adam(learning_rate=.0015)\n",
    "network.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "obs = env.reset()\n",
    "epi_duration = 0\n",
    "epi = 0 # number of episode taken\n",
    "epsilon=1.0\n",
    "\n",
    "for t in notebook.tqdm(range(1, train_nums+1), desc='train with DQN'):\n",
    "    # epsilon update\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "    #######################  step 1  ####################### \n",
    "    ####        Select action using episolon-greedy      ### \n",
    "    ########################################################   \n",
    "\n",
    "    \n",
    "    #######################  step 2  ####################### \n",
    "    #### Take step and store transition to replay buffer ### \n",
    "    ########################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #######################  step 3  ####################### \n",
    "    ####     Train network (perform gradient descent)    ### \n",
    "    ########################################################\n",
    "    \n",
    "    \n",
    "\n",
    "    #######################  step 3  ####################### \n",
    "    ####             Update target network               ### \n",
    "    ########################################################\n",
    "      \n",
    "\n",
    "    obs = next_obs  # s <- s'\n",
    "    # if episode ends (done)\n",
    "    if (epi_duration >= max_iter) or done:\n",
    "        epi += 1 # num of episode \n",
    "        print(\"[Episode {:>5}] epi duration: {:>6.2f}  --eps : {:>4.2f} --steps : {:>5}\".format(epi, epi_duration, epsilon, t))\n",
    "        obs, done, epi_duration = env.reset(), False, 0  # Environmnet reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = env.chronics_handler.max_timestep()\n",
    "\n",
    "# test before train\n",
    "for i in range(5):\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    actions = []\n",
    "    for t in notebook.tqdm(range(max_steps), desc=\"\"):\n",
    "        converted_obs = agent.convert_obs(obs)\n",
    "        action = network.action_value(np.atleast_2d(converted_obs))\n",
    "        actions.append(action)\n",
    "        converted_act = agent.id_to_act(action)\n",
    "        obs, reward, done, _ = env.step(converted_act)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
